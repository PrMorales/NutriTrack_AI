import streamlit as st
import os
from google import genai
from google.genai import types
import pandas as pd
import datetime
# (Nﾃ｣o precisamos mais de 'import time')

# --- 1. CONFIGURAﾃﾃグ DE SEGURANﾃ② E IA (GEMINI) ---
try:
    # Lﾃｪ a chave da variﾃ｡vel de ambiente GEMINI_API_KEY
    chave_secreta = os.environ.get("GEMINI_API_KEY") 

    if not chave_secreta:
        st.error("Erro: A chave GEMINI_API_KEY nﾃ｣o foi encontrada. Configure nos segredos do Streamlit Cloud.")
        st.stop()
        
    client = genai.Client(api_key=chave_secreta)
    
except Exception as e:
    # Isso pode pegar erros como problemas de conexﾃ｣o de rede ou autenticaﾃｧﾃ｣o
    st.error(f"Erro Fatal na Inicializaﾃｧﾃ｣o da API Gemini. Verifique os logs. {e}")
    st.stop()
    
# --- CONFIGURAﾃﾃグ INICIAL DA Pﾃ；INA E TEMA ---
st.set_page_config(page_title="NutriTrack IA - Vegetariano", layout="wide")

# Personalizando a interface para parecer mais limpa (como o Gemini/ChatGPT)
st.markdown(
    """
    <style>
    /* Esconde o menu e o rodapﾃｩ padrﾃ｣o do Streamlit */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stApp {
        padding-top: 20px;
        padding-bottom: 20px;
    }
    .st-emotion-cache-1r6r8qj {
        /* Centraliza o conteﾃｺdo principal */
        max-width: 800px;
    }
    </style>
    """, unsafe_allow_html=True
)

st.title("験 NutriTrack IA: Anﾃ｡lise Vegetariana Inteligente")
st.subheader("Informe sua refeiﾃｧﾃ｣o e receba a anﾃ｡lise nutricional e dicas!")

# Inicializa o histﾃｳrico do chat
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "model", "content": "Olﾃ｡! Eu sou seu assistente NutriTrack IA. Informe-me o que vocﾃｪ comeu (ex: 'Lentilha, arroz e salada de tomate') para receber a anﾃ｡lise e dicas vegetarianas."}
    ]


# --- FUNﾃﾃグ DE CONVERSA COM A IA (CORRIGIDA E FOCADA) ---
def conversar_com_ia(prompt):
    
    # INSTRUﾃﾃグ DETALHADA PARA FORﾃ②R A SAﾃ好A EM TABELA E DAR DICAS VEGETARIANAS
    system_instruction = (
        "Vocﾃｪ ﾃｩ o NutriTrack AI, um especialista em nutriﾃｧﾃ｣o focado em dietas vegetarianas. "
        "1. Analise o alimento ou refeiﾃｧﾃ｣o fornecida pelo usuﾃ｡rio. "
        "2. Retorne uma tabela formatada em MARKDOWN com 7 colunas (Calorias, Aﾃｧﾃｺcar, Vitamina C, Proteﾃｭna, Ferro, Carboidratos, Gorduras). "
        "3. Sempre adicione uma breve dica de alimentaﾃｧﾃ｣o focada em vegetarianos, especialmente sobre como obter nutrientes como Ferro e Proteﾃｭna."
        "4. Mantenha um tom profissional e amigﾃ｡vel. Nﾃグ use nomes de pessoas ou pratos especﾃｭficos de usuﾃ｡rio."
    )
    
    # Prepara o histﾃｳrico (CORREﾃﾃグ DE TypeError implementada)
    history = []
    for m in st.session_state.messages:
        # Garante que sﾃｳ mensagens vﾃ｡lidas com 'content' sejam enviadas
        if 'content' in m and m['content'] and 'role' in m:
            history.append(
                types.Content(
                    role="user" if m["role"] == "user" else "model", 
                    parts=[types.Part.from_text(m["content"])]
                )
            )

    # Configuraﾃｧﾃｵes e chamada do modelo
    config = types.GenerateContentConfig(
        system_instruction=system_instruction
    )

    response = client.models.generate_content_stream(
        model='gemini-2.5-flash', # Modelo rﾃ｡pido e eficiente do Gemini
        contents=history + [types.Content(role="user", parts=[types.Part.from_text(prompt)])],
        config=config
    )
    return response

# ----------------------------------------------------
# COLUNA PRINCIPAL: CHATBOT
# ----------------------------------------------------

# Exibir histﾃｳrico de mensagens
for message in st.session_state.messages:
    # Ajuste de roles para o Streamlit (model -> assistant)
    role = "assistant" if message["role"] == "model" else "user" 
    with st.chat_message(role):
        st.markdown(message["content"])

# Capturar novo input do usuﾃ｡rio
if prompt := st.chat_input("Ex: 'Sanduﾃｭche de pasta de amendoim com banana'"):
    
    # 1. Exibir input do usuﾃ｡rio
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. Gerar e exibir resposta da IA (streaming)
    with st.chat_message("assistant"):
        with st.spinner("Analisando refeiﾃｧﾃ｣o e gerando tabela nutricional..."):
            try:
                response_stream = conversar_com_ia(prompt)
                full_response = st.write_stream(response_stream)
            except Exception as e:
                # Exibe um erro amigﾃ｡vel ao usuﾃ｡rio se a API falhar
                st.error("Erro na comunicaﾃｧﾃ｣o com a IA. Tente novamente ou verifique sua chave GEMINI_API_KEY.")
                # O erro completo ainda ﾃｩ registrado no log do Streamlit Cloud
                full_response = "Erro de API." 
        
    # 3. Adicionar resposta completa ao histﾃｳrico
    # O Gemini usa 'model'
    st.session_state.messages.append({"role": "model", "content": full_response})