import streamlit as st
import os
from google import genai
from google.genai import types
import pandas as pd
import datetime
# (N√£o precisamos mais de 'import time')

# --- 1. CONFIGURA√á√ÉO DE SEGURAN√áA E IA (GEMINI) ---
try:
    # L√™ a chave da vari√°vel de ambiente GEMINI_API_KEY
    chave_secreta = os.environ.get("GEMINI_API_KEY") 

    if not chave_secreta:
        st.error("Erro: A chave GEMINI_API_KEY n√£o foi encontrada. Configure nos segredos do Streamlit Cloud.")
        st.stop()
        
    client = genai.Client(api_key=chave_secreta)
    
except Exception as e:
    # Isso pode pegar erros como problemas de conex√£o de rede ou autentica√ß√£o
    st.error(f"Erro Fatal na Inicializa√ß√£o da API Gemini. Verifique os logs. {e}")
    st.stop()
    
# --- CONFIGURA√á√ÉO INICIAL DA P√ÅGINA E TEMA ---
st.set_page_config(page_title="NutriTrack IA - Vegetariano", layout="wide")

# Personalizando a interface para parecer mais limpa (como o Gemini/ChatGPT)
st.markdown(
    """
    <style>
    /* Esconde o menu e o rodap√© padr√£o do Streamlit */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    .stApp {
        padding-top: 20px;
        padding-bottom: 20px;
    }
    .st-emotion-cache-1r6r8qj {
        /* Centraliza o conte√∫do principal */
        max-width: 800px;
    }
    </style>
    """, unsafe_allow_html=True
)

st.title("üå± NutriTrack IA: An√°lise Vegetariana Inteligente")
st.subheader("Informe sua refei√ß√£o e receba a an√°lise nutricional e dicas!")

# Inicializa o hist√≥rico do chat
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "model", "content": "Ol√°! Eu sou seu assistente NutriTrack IA. Informe-me o que voc√™ comeu (ex: 'Lentilha, arroz e salada de tomate') para receber a an√°lise e dicas vegetarianas."}
    ]


# --- FUN√á√ÉO DE CONVERSA COM A IA (CORRIGIDA E FOCADA) ---
def conversar_com_ia(prompt):
    
    # üåü A INSTRU√á√ÉO DO SISTEMA √â O QUE DEFINE O BOT
    system_instruction = (
        "Voc√™ √© o NutriTrack AI, um especialista em nutri√ß√£o focado em dietas vegetarianas. "
        "1. Analise o alimento ou refei√ß√£o fornecida pelo usu√°rio. "
        "2. Retorne uma tabela formatada em MARKDOWN com 7 colunas (Calorias, A√ß√∫car, Vitamina C, Prote√≠na, Ferro, Carboidratos, Gorduras). "
        "3. Sempre adicione uma breve dica de alimenta√ß√£o focada em vegetarianos, especialmente sobre como obter nutrientes como Ferro e Prote√≠na."
        "4. Mantenha um tom profissional e amig√°vel."
    )
    
    # Prepara o hist√≥rico e o novo prompt, incluindo a instru√ß√£o do sistema como a primeira parte
    # O Gemini SDK mais recente prefere essa estrutura para a instru√ß√£o.
    contents_to_send = []
    
    # Adiciona a instru√ß√£o do sistema
    contents_to_send.append(
        types.Content(
            role="user",
            parts=[types.Part.from_text(system_instruction)]
        )
    )
    
    # Adiciona o hist√≥rico da conversa (para manter o contexto)
    for m in st.session_state.messages:
        if 'content' in m and m['content'] and 'role' in m:
            # Garante que o role seja 'user' ou 'model' (formato Gemini)
            role = "user" if m["role"] == "user" else "model"
            contents_to_send.append(
                types.Content(
                    role=role, 
                    parts=[types.Part.from_text(m["content"])]
                )
            )

    # Adiciona a mensagem atual do usu√°rio
    contents_to_send.append(
        types.Content(role="user", parts=[types.Part.from_text(prompt)])
    )

    # Chama a API do Gemini com o modelo r√°pido e a lista de conte√∫dos
    response = client.models.generate_content_stream(
        model='gemini-2.5-flash',
        contents=contents_to_send
        # Removemos o 'config' desnecess√°rio
    )
    return response
# ----------------------------------------------------
# COLUNA PRINCIPAL: CHATBOT
# ----------------------------------------------------

# Exibir hist√≥rico de mensagens
for message in st.session_state.messages:
    # Ajuste de roles para o Streamlit (model -> assistant)
    role = "assistant" if message["role"] == "model" else "user" 
    with st.chat_message(role):
        st.markdown(message["content"])

# Capturar novo input do usu√°rio
if prompt := st.chat_input("Ex: 'Sandu√≠che de pasta de amendoim com banana'"):
    
    # 1. Exibir input do usu√°rio
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. Gerar e exibir resposta da IA (streaming)
    with st.chat_message("assistant"):
        with st.spinner("Analisando refei√ß√£o e gerando tabela nutricional..."):
            try:
                response_stream = conversar_com_ia(prompt)
                full_response = st.write_stream(response_stream)
            except Exception as e:
                # Exibe um erro amig√°vel ao usu√°rio se a API falhar
                st.error("Erro na comunica√ß√£o com a IA. Tente novamente ou verifique sua chave GEMINI_API_KEY.")
                # O erro completo ainda √© registrado no log do Streamlit Cloud
                full_response = "Erro de API." 
        
    # 3. Adicionar resposta completa ao hist√≥rico
    # O Gemini usa 'model'
    st.session_state.messages.append({"role": "model", "content": full_response})